{
    "dataset": {
        "dir": "input/data-science-bowl-2019/",
        "feature_dir": "features",
        "params": null
    },
    "av": {
        "params": {},
        "split_params": {
            "n_splits": 5,
            "random_state": 42
        },
        "model_params": {
            "objective": "binary",
            "metric": "auc",
            "boosting": "gbdt",
            "max_depth": 7,
            "num_leaves": 75,
            "learning_rate": 0.01,
            "colsample_bytree": 0.7,
            "subsample": 0.1,
            "subsample_freq": 1,
            "seed": 111,
            "feature_fraction_seed": 111,
            "drop_seed": 111,
            "verbose": -1,
            "n_jobs": -1,
            "first_metric_only": true
        },
        "train_params": {
            "num_boost_round": 50000,
            "early_stopping_rounds": 200,
            "verbose_eval": 200
        }
    },
    "model": {
        "name": "nn",
        "sampling": {
            "name": "none",
            "params": {}
        },
        "model_params": {
            "emb_drop": 0.3,
            "drop": 0.5
        },
        "train_params": {
            "scheduler": {
                "name": "cosine",
                "T_max": 10,
                "eta_min": 1e-05
            },
            "batch_size": 256,
            "n_epochs": 50,
            "lr": 0.001
        },
        "mode": "ovr",
        "save_path": "output/nn_11_valid_50_percentile_0.80reduction/pth/",
        "policy": "best_score"
    },
    "post_process": {
        "params": {
            "reverse": false,
            "n_overall": 20,
            "n_classwise": 20
        }
    },
    "val": {
        "name": "group_kfold",
        "params": {
            "n_splits": 5,
            "random_state": 111
        },
        "percentile": 70,
        "n_delete": 0.95
    },
    "output_dir": "output",
    "features": [
        "PastSummary3",
        "NakamaV8"
    ],
    "args": {
        "config": "config/nn_14_valid_70_percentile_0.95reduction.yml"
    },
    "model_output_dir": "output/nn_14_valid_70_percentile_0.95reduction",
    "av_result": {
        "score": {
            "fold0": {
                "train": {
                    "auc": 0.8133600277830316
                },
                "valid": {
                    "auc": 0.5214999466788908
                }
            },
            "fold1": {
                "train": {
                    "auc": 0.7130761789227372
                },
                "valid": {
                    "auc": 0.5969492862981638
                }
            },
            "fold2": {
                "train": {
                    "auc": 0.7510484000376266
                },
                "valid": {
                    "auc": 0.5274029718786734
                }
            },
            "fold3": {
                "train": {
                    "auc": 0.6848564654056827
                },
                "valid": {
                    "auc": 0.5574729204479798
                }
            },
            "fold4": {
                "train": {
                    "auc": 0.9404856196341628
                },
                "valid": {
                    "auc": 0.5584443138312529
                }
            }
        },
        "feature_importances": {
            "nunique_event_count": 1092.7132428712137,
            "Ratio_4070_Counter": 865.6531610842105,
            "Ratio_2030_Counter": 838.1174146789934,
            "Ratio_4020_Counter": 744.669851349503,
            "Ratio_4025_Counter": 743.3076690965455,
            "4070": 690.2731693498623,
            "Ratio_Game_Counter": 657.90969776862,
            "mean_action_time_All Star Sorting": 635.6542506480961,
            "Ratio_3021_Counter": 602.7099544739028,
            "mean_action_time_Happy Camel": 599.4945336226583,
            "Ratio_3121_Counter": 577.9580739826206,
            "Ratio_4090_Counter": 576.5658865691353,
            "mean_action_time_Scrub-A-Dub": 556.6931331982678,
            "Ratio_3020_Counter": 524.5023225659165,
            "Chow Time_4070": 521.2576307924608,
            "mean_target": 516.0495834287593,
            "Ratio_Sandcastle_Builder__Activity__4020_Counter": 468.711064269891,
            "Ratio_2010_Counter": 458.42583495140354,
            "Cauldron Filler (Assessment)_mean_action_time": 453.16524038332886,
            "Ratio_Crystal_Caves___Level_3_2000_Counter": 448.1177341401667,
            "Ratio_3120_Counter": 404.2159572740076,
            "2000": 373.9265938018837,
            "27253bdc": 373.694587265111,
            "success_ratio_Scrub-A-Dub": 352.74991657551146,
            "mean_correct_Chow Time": 292.5146464258756,
            "Chest Sorter (Assessment)_mean_action_time": 286.0773043282019,
            "total_accuracy": 284.6911790981682,
            "Chest Sorter (Assessment)_mean_var_action_time": 269.6553473896009,
            "total_accuracy_7day_Bird_Measurer__Assessment_": 257.0632480487533,
            "accuracy_group_mean": 250.41932207001537,
            "mean_incorrect_All Star Sorting": 244.4596485712551,
            "accuracy_group_mean_7day": 242.91955652962204,
            "success_ratio_Happy Camel": 238.5372681334371,
            "Mushroom Sorter (Assessment)_success_ratio": 225.06491714677296,
            "accumulated_acc": 224.0307051183927,
            "mean_incorrect_Pan Balance": 217.62368136607802,
            "Clip_Counter": 205.95674660171179,
            "total_accuracy_7day": 202.38081097653668,
            "Ratio_Chest_Sorter__Assessment__4020_Counter": 195.33562055103175,
            "num_correct_mean": 191.28148713911432,
            "total_accuracy_7day_Mushroom_Sorter__Assessment_": 191.1496068953489,
            "Ratio_Chest_Sorter__Assessment__4025_Counter": 186.48477192882464,
            "accumulated_accuracy_group": 182.1196960512967,
            "Cauldron Filler (Assessment)_3020_mean": 165.49122723956063,
            "total_accuracy_Chest_Sorter__Assessment_": 145.01411158672627,
            "var_action_time_Scrub-A-Dub": 144.56647856482982,
            "mean_correct_Leaf Leader": 140.42726427826946,
            "last_success_ratio_Crystals Rule": 136.32572776847974,
            "session_title": 126.31970500036368,
            "num_incorrect_title_mean": 114.7922318392527,
            "success_ratio_Pan Balance": 113.34121162539515,
            "Mushroom Sorter (Assessment)_accuracy_group": 111.96414001404736,
            "accuracy_group_median": 98.48394548994838,
            "mean_timte_to_get_success_same_assess": 85.32578004238638,
            "decayed_accuracy_group_last_same_assess": 82.85624616490395,
            "world": 78.01371319891405,
            "last_success_ratio_Pan Balance": 75.71387867807285,
            "Crystal Caves - Level 3_2000": 74.56140486973209,
            "Crystal_Caves___Level_3_2000_Counter": 67.35111224068096,
            "title": 61.89571031735977,
            "accuracy_group_title_mean": 56.407210560720884,
            "decayed_success_ratio_last_same_assess": 56.343991893919885,
            "success_ratio_same_assess": 56.039567326679595,
            "accuracy_group_title_7day_last": 35.71584353480139,
            "num_incorrect_title_median": 26.40493652875098,
            "mean_accuracy_group_same_assess": 19.850524638913342,
            "accuracy_group_title_last": 18.200241484584332,
            "num_correct_title_mean": 17.90165827226374,
            "success_ratio_last_same_assess": 16.29984510038048,
            "accuracy_group_title_median": 12.85075954345084,
            "num_correct_title_last": 1.5483519554138183
        }
    },
    "eval_results": {
        "evals_result": {
            "oof_score": 0.5637953402198715,
            "normal_oof_score": 0.6068998444235019,
            "truncated_eval_mean": 0.5637732111587428,
            "truncated_eval_0.95upper": 0.5821097193206443,
            "truncated_eval_0.95lower": 0.5454367029968413,
            "truncated_eval_std": 0.00916825408095072,
            "cv_score": {
                "cv1": {
                    "loss": 0.42384906113147736,
                    "qwk": 0.5969505574934391
                },
                "cv2": {
                    "loss": 0.42936885356903076,
                    "qwk": 0.5642121886432307
                },
                "cv3": {
                    "loss": 0.420511856675148,
                    "qwk": 0.5649040295067089
                },
                "cv4": {
                    "loss": 0.42704878002405167,
                    "qwk": 0.582384377004034
                },
                "cv5": {
                    "loss": 0.4303957000374794,
                    "qwk": 0.5475253587769013
                }
            },
            "n_data": 17690,
            "n_features": 71
        },
        "feature_importance": {
            "title": 0.8189100585192298,
            "total_accuracy_7day_Mushroom_Sorter__Assessment_": 0.782180626426244,
            "total_accuracy_7day_Bird_Measurer__Assessment_": 0.7809395130048093,
            "total_accuracy_7day": 0.6579923077991979,
            "total_accuracy": 0.649960469835397,
            "total_accuracy_Chest_Sorter__Assessment_": 0.6498505846038144,
            "nunique_event_count": 0.6438788415344558,
            "num_incorrect_title_median": 0.5098423817511506,
            "num_incorrect_title_mean": 0.5037434397828335,
            "num_correct_title_mean": 0.4995253987119609,
            "num_correct_title_last": 0.49404946684808476,
            "accuracy_group_title_median": 0.47980181121752014,
            "accuracy_group_title_last": 0.47960214964736014,
            "num_correct_mean": 0.4788888779945144,
            "accuracy_group_title_mean": 0.47708493595079665,
            "accuracy_group_title_7day_last": 0.47502458662464725,
            "accuracy_group_mean": 0.47397411714306725,
            "Ratio_Sandcastle_Builder__Activity__4020_Counter": 0.471825371280236,
            "accuracy_group_mean_7day": 0.47059166954294585,
            "accuracy_group_median": 0.47050016731581873,
            "Ratio_Game_Counter": 0.46315906864224,
            "Ratio_Crystal_Caves___Level_3_2000_Counter": 0.4590172215246143,
            "Ratio_Chest_Sorter__Assessment__4025_Counter": 0.4524551990434539,
            "Ratio_4090_Counter": 0.45178983642169507,
            "Ratio_Chest_Sorter__Assessment__4020_Counter": 0.4517652716424072,
            "Ratio_4070_Counter": 0.4454167789654571,
            "Ratio_4020_Counter": 0.43008400726806073,
            "Ratio_4025_Counter": 0.4287723738291033,
            "Ratio_3121_Counter": 0.4113249139856534,
            "Ratio_3120_Counter": 0.40922710505834636,
            "Ratio_3021_Counter": 0.39977989184302987,
            "Ratio_3020_Counter": 0.3869027211236543,
            "Ratio_2030_Counter": 0.3784846239298272,
            "Crystal_Caves___Level_3_2000_Counter": 0.3776945816269127,
            "Ratio_2010_Counter": 0.37707995952088436,
            "Clip_Counter": 0.3689157230498262,
            "mean_target": 0.3631945426477493,
            "Mushroom Sorter (Assessment)_success_ratio": 0.07098862523374125,
            "var_action_time_Scrub-A-Dub": 0.0689911205930885,
            "mean_action_time_Scrub-A-Dub": 0.0649280518042565,
            "success_ratio_Scrub-A-Dub": 0.06385191688967957,
            "last_success_ratio_Pan Balance": 0.060834031843767636,
            "success_ratio_Pan Balance": 0.06014082940126449,
            "mean_incorrect_Pan Balance": 0.05841234173931102,
            "mean_correct_Leaf Leader": 0.0528058670661949,
            "mean_action_time_Happy Camel": 0.05253026320085206,
            "success_ratio_Happy Camel": 0.04981121592781075,
            "mean_correct_Chow Time": 0.045239123461295196,
            "last_success_ratio_Crystals Rule": 0.044652302727475425,
            "mean_incorrect_All Star Sorting": 0.04309087017834441,
            "mean_action_time_All Star Sorting": 0.0430228096934326,
            "Chest Sorter (Assessment)_mean_action_time": 0.03828250064634024,
            "Chest Sorter (Assessment)_mean_var_action_time": 0.03809742215760683,
            "Cauldron Filler (Assessment)_3020_mean": 0.03709557305364804,
            "Cauldron Filler (Assessment)_mean_action_time": 0.036984844388434635,
            "decayed_accuracy_group_last_same_assess": 0.03548507887350345,
            "decayed_success_ratio_last_same_assess": 0.034434689799548335,
            "Mushroom Sorter (Assessment)_accuracy_group": 0.03330896612354879,
            "mean_timte_to_get_success_same_assess": 0.03170695702481683,
            "success_ratio_last_same_assess": 0.03126146368104752,
            "mean_accuracy_group_same_assess": 0.02827812968573362,
            "success_ratio_same_assess": 0.027362726248040238,
            "accumulated_accuracy_group": 0.026516400213006653,
            "session_title": 0.02397658973815644,
            "world": 0.023974460154702816,
            "Chow Time_4070": 0.021392973974062656,
            "accumulated_acc": 0.021289297368385317,
            "Crystal Caves - Level 3_2000": 0.01882011630956255,
            "27253bdc": 0.014451633338359703,
            "4070": 0.007362204348848378,
            "2000": 0.004486349403051038
        }
    },
    "truncated_mean_adjust": 0.5696617321984584,
    "truncated_std_adjust": 0.022223855915961804,
    "truncated_upper": 0.614109444030382,
    "truncated_lower": 0.5252140203665348
}