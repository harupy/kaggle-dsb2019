{
    "dataset": {
        "dir": "input/data-science-bowl-2019/",
        "feature_dir": "features",
        "params": null
    },
    "av": {
        "params": {},
        "split_params": {
            "n_splits": 5,
            "random_state": 42
        },
        "model_params": {
            "objective": "binary",
            "metric": "auc",
            "boosting": "gbdt",
            "max_depth": 7,
            "num_leaves": 75,
            "learning_rate": 0.01,
            "colsample_bytree": 0.7,
            "subsample": 0.1,
            "subsample_freq": 1,
            "seed": 111,
            "feature_fraction_seed": 111,
            "drop_seed": 111,
            "verbose": -1,
            "n_jobs": -1,
            "first_metric_only": true
        },
        "train_params": {
            "num_boost_round": 50000,
            "early_stopping_rounds": 200,
            "verbose_eval": 200
        }
    },
    "model": {
        "name": "nn",
        "sampling": {
            "name": "none",
            "params": {}
        },
        "model_params": {
            "emb_drop": 0.3,
            "drop": 0.5
        },
        "train_params": {
            "scheduler": {
                "name": "cosine",
                "T_max": 10,
                "eta_min": 1e-05
            },
            "batch_size": 256,
            "n_epochs": 50,
            "lr": 0.001
        },
        "mode": "ovr",
        "save_path": "output/nn_16_valid_80_percentile_0.90reduction/pth/",
        "policy": "best_score"
    },
    "post_process": {
        "params": {
            "reverse": false,
            "n_overall": 20,
            "n_classwise": 20
        }
    },
    "val": {
        "name": "group_kfold",
        "params": {
            "n_splits": 5,
            "random_state": 111
        },
        "percentile": 80,
        "n_delete": 0.9
    },
    "output_dir": "output",
    "features": [
        "PastSummary3",
        "NakamaV8"
    ],
    "args": {
        "config": "config/nn_16_valid_80_percentile_0.90reduction.yml"
    },
    "model_output_dir": "output/nn_16_valid_80_percentile_0.90reduction",
    "av_result": {
        "score": {
            "fold0": {
                "train": {
                    "auc": 0.8826819725748677
                },
                "valid": {
                    "auc": 0.5394082789378435
                }
            },
            "fold1": {
                "train": {
                    "auc": 0.7029374085215543
                },
                "valid": {
                    "auc": 0.5463077636540475
                }
            },
            "fold2": {
                "train": {
                    "auc": 0.6502180906691456
                },
                "valid": {
                    "auc": 0.5288209270627162
                }
            },
            "fold3": {
                "train": {
                    "auc": 0.8437972238892916
                },
                "valid": {
                    "auc": 0.5743735665053955
                }
            },
            "fold4": {
                "train": {
                    "auc": 0.8645126641291996
                },
                "valid": {
                    "auc": 0.5600415975328764
                }
            }
        },
        "feature_importances": {
            "nunique_event_count": 395.5752341542484,
            "mean_action_time_Dino Drink": 286.260580650759,
            "Ratio_4010_Counter": 273.5410008469455,
            "mean_action_time_All Star Sorting": 255.736458604707,
            "Ratio_4090_Counter": 242.6036992343512,
            "Ratio_4070_Counter": 237.30136008967784,
            "Ratio_4020_Counter": 236.18908509005277,
            "Sandcastle Builder (Activity)_duration": 213.0711954188846,
            "Ratio_2030_Counter": 210.30582803166325,
            "Ratio_2000_Counter": 207.6425523593134,
            "Ratio_4025_Counter": 206.06161509376668,
            "Ratio_Scrub_A_Dub_3020_Counter": 196.72568046369778,
            "Ratio_Bottle_Filler__Activity__4070_Counter": 194.9579022474456,
            "Ratio_Crystal_Caves___Level_1_2000_Counter": 194.78057423559724,
            "Ratio_3010_Counter": 190.11157556426377,
            "Ratio_3021_Counter": 185.49471864145016,
            "success_ratio_Chow Time": 184.7577066544727,
            "Ratio_Cart_Balancer__Assessment__4070_Counter": 184.2385252968935,
            "Ratio_4035_Counter": 180.6986078824346,
            "Ratio_Sandcastle_Builder__Activity__4020_Counter": 178.77600303544034,
            "Ratio_Game_Counter": 175.91738894062675,
            "Ratio_3020_Counter": 169.8854339060792,
            "Ratio_Activity_Counter": 169.39770359536982,
            "CRYSTALCAVES_Counter": 166.6186256049854,
            "Ratio_TREETOPCITY_Counter": 166.0449029724379,
            "Cauldron Filler (Assessment)_mean_action_time": 161.43405625085944,
            "4035": 159.01941207183938,
            "4070": 158.33052852848124,
            "Ratio_Crystal_Caves___Level_2_2000_Counter": 158.17692985077358,
            "Ratio_2010_Counter": 156.9993079011132,
            "Ratio_3121_Counter": 156.81579975055038,
            "mean_action_time_Happy Camel": 154.78133835914196,
            "Ratio_Bug_Measurer__Activity__4035_Counter": 153.66883587579179,
            "Ratio_3110_Counter": 152.820929714691,
            "num_incorrect_mean": 150.77443208446311,
            "Cauldron Filler (Assessment)_mean_var_action_time": 149.4619028911664,
            "Ratio_Crystal_Caves___Level_3_2000_Counter": 148.4293894690045,
            "Ratio_Chow_Time_4070_Counter": 146.84460457735776,
            "mean_action_time_Scrub-A-Dub": 144.51708120876827,
            "Chow Time_4070": 143.63901930450987,
            "mean_target": 140.8202436145795,
            "Ratio_Tree_Top_City___Level_3_2000_Counter": 134.69160584883173,
            "Ratio_Tree_Top_City___Level_1_2000_Counter": 134.1621039314894,
            "Ratio_4030_Counter": 133.96849668150463,
            "Ratio_Scrub_A_Dub_4010_Counter": 125.38045294954935,
            "Ratio_CRYSTALCAVES_Counter": 121.8682904337238,
            "nunique_title": 119.64572757579663,
            "Ratio_Ordering_Spheres_2000_Counter": 117.5320229733741,
            "total_accuracy_Bird_Measurer__Assessment_": 112.58915885811021,
            "Chest Sorter (Assessment)_mean_action_time": 110.12982793875199,
            "Ratio_3120_Counter": 108.36187413950103,
            "2000": 107.82941370453035,
            "accuracy_group_mean": 100.99423685154687,
            "Ratio_All_Star_Sorting_3120_Counter": 97.87282195927027,
            "success_ratio_Scrub-A-Dub": 93.80536578518404,
            "27253bdc": 93.0168462177684,
            "Ratio_Bird_Measurer__Assessment__3121_Counter": 92.75937788209072,
            "Chest Sorter (Assessment)_mean_var_action_time": 90.19475209658586,
            "mean_4070_Chow Time": 89.47536934779265,
            "3120": 88.43399482574318,
            "mean_correct_Chow Time": 83.66080571192143,
            "Ratio_Happy_Camel_4070_Counter": 81.96930681123776,
            "Chow_Time_4070_Counter": 81.26890703982208,
            "success_ratio_All Star Sorting": 80.90289274945752,
            "total_accuracy_7day": 80.40692243838384,
            "Ratio_Cauldron_Filler__Assessment__4025_Counter": 77.9337480901807,
            "4070_Counter": 72.7349244709948,
            "total_accuracy_Mushroom_Sorter__Assessment_": 72.63423861101327,
            "total_accuracy": 72.43210928407788,
            "total_accuracy_7day_Bird_Measurer__Assessment_": 72.39297525203582,
            "nunique_hour": 70.68472836042638,
            "num_correct_mean": 69.73915258997278,
            "Chest Sorter (Assessment)_success_ratio": 69.42971746459371,
            "accumulated_acc": 67.18811937536938,
            "accuracy_group_mean_7day": 63.267535794380095,
            "Ratio_Chest_Sorter__Assessment__4020_Counter": 62.051863356716794,
            "Ratio_Mushroom_Sorter__Assessment__2010_Counter": 60.97163789169413,
            "3020_Counter": 58.912918772411096,
            "mean_incorrect_Pan Balance": 58.7750215003427,
            "mean_correct_Leaf Leader": 58.70504912489996,
            "mean_action_time_last_same_assess": 57.69363508942479,
            "Mushroom Sorter (Assessment)_success_ratio": 57.67394367889492,
            "Ratio_Bird_Measurer__Assessment__2030_Counter": 56.556423087884475,
            "Clip_Counter": 54.218976320832006,
            "success_ratio_Happy Camel": 53.285999266212094,
            "Ratio_Air_Show_2030_Counter": 52.320783609620406,
            "success_ratio_Pan Balance": 50.91207155020902,
            "mean_incorrect_All Star Sorting": 48.85974000587175,
            "total_accuracy_Cauldron_Filler__Assessment_": 48.613250796640926,
            "total_accuracy_7day_Mushroom_Sorter__Assessment_": 48.370018955434716,
            "Cauldron Filler (Assessment)_3020_mean": 47.882832285974295,
            "Ratio_Air_Show_3021_Counter": 47.58361096893204,
            "2000_Counter": 47.39712077811273,
            "success_ratio_Air Show": 45.05108084563399,
            "Ratio_Chest_Sorter__Assessment__4025_Counter": 44.60911887310649,
            "nunique_world": 42.60821025371551,
            "Ratio_Chest_Sorter__Assessment__4030_Counter": 42.56972389169169,
            "last_success_ratio_All Star Sorting": 42.33832726478577,
            "accumulated_accuracy_group": 42.09636212181358,
            "last_success_ratio_Happy Camel": 40.469031688506945,
            "Mushroom Sorter (Assessment)_accuracy_group": 38.15525942476961,
            "num_correct_mean_7day": 36.320120211468016,
            "session_title": 34.966776406764986,
            "decayed_accuracy_group_last_same_assess": 33.28990721702576,
            "Ratio_Happy_Camel_4035_Counter": 32.95912924353761,
            "last_success_ratio_Crystals Rule": 32.802763202133065,
            "mean_timte_to_get_success_same_assess": 32.470917021572916,
            "total_accuracy_Chest_Sorter__Assessment_": 32.32019097988778,
            "var_action_time_Scrub-A-Dub": 30.579247382146967,
            "num_incorrect_title_mean": 30.14152983530439,
            "Crystal_Caves___Level_3_2000_Counter": 26.54276732723592,
            "last_success_ratio_Pan Balance": 23.604715812288852,
            "n_last_correct_Leaf Leader": 21.712467741966247,
            "var_time_to_get_success_same_assess": 20.536555881520325,
            "accuracy_group_title_sum": 19.732281720638277,
            "accuracy_group_median": 19.278326062386622,
            "n_failure_same_assess": 17.97221856201904,
            "Chest Sorter (Assessment)_time_to_get_success": 17.680004659469706,
            "num_incorrect_title_median": 17.329874484427272,
            "Crystal Caves - Level 3_2000": 16.20806235941709,
            "Ratio_Air_Show_3121_Counter": 16.10496424466837,
            "title": 11.854341888427733,
            "decayed_success_ratio_last_same_assess": 11.679224699735641,
            "num_correct_title_7day_mean": 10.22739598751068,
            "Ratio_Chest_Sorter__Assessment__3021_Counter": 9.832505023479461,
            "accuracy_group_title_7day_mean": 8.300662813754752,
            "success_ratio_same_assess": 7.7760612276393655,
            "Crystal_Caves___Level_3_Counter": 7.661732101440429,
            "accuracy_group_title_mean": 7.362852821403066,
            "decayed_n_failure_last_same_assess": 7.061542660907875,
            "accuracy_group_title_max": 5.173686003684997,
            "world": 4.923348593574792,
            "accuracy_group_title_7day_last": 4.305519436651958,
            "success_ratio_last_same_assess": 4.148114013671876,
            "accuracy_group_title_median": 3.8884698152542114,
            "num_correct_title_mean": 3.113476539729163,
            "mean_accuracy_group_same_assess": 2.8836265520112647,
            "accuracy_group_title_last": 6.817760004196316e-06,
            "num_correct_title_median": 0.0,
            "num_correct_title_last": 0.0,
            "num_correct_title_7day_last": 0.0
        }
    },
    "eval_results": {
        "evals_result": {
            "oof_score": 0.5739106548725484,
            "normal_oof_score": 0.6089556122759892,
            "truncated_eval_mean": 0.5629603353554318,
            "truncated_eval_0.95upper": 0.5794972939516213,
            "truncated_eval_0.95lower": 0.5464233767592422,
            "truncated_eval_std": 0.008268479298094782,
            "cv_score": {
                "cv1": {
                    "loss": 0.41439277678728104,
                    "qwk": 0.6151744454804591
                },
                "cv2": {
                    "loss": 0.42765195667743683,
                    "qwk": 0.5721673649278702
                },
                "cv3": {
                    "loss": 0.4217647910118103,
                    "qwk": 0.5650546990335619
                },
                "cv4": {
                    "loss": 0.42435216903686523,
                    "qwk": 0.5795136745418361
                },
                "cv5": {
                    "loss": 0.4249442592263222,
                    "qwk": 0.5814821146202829
                }
            },
            "n_data": 17690,
            "n_features": 141
        },
        "feature_importance": {
            "Ratio_Air_Show_3021_Counter": 0.5771514697631897,
            "Ratio_Air_Show_3121_Counter": 0.5754028565168842,
            "title": 0.5579853238846748,
            "total_accuracy_7day_Bird_Measurer__Assessment_": 0.5431165910683842,
            "total_accuracy_Bird_Measurer__Assessment_": 0.5420712305794484,
            "total_accuracy_7day_Mushroom_Sorter__Assessment_": 0.5418393785619485,
            "total_accuracy_Mushroom_Sorter__Assessment_": 0.5403678854279399,
            "total_accuracy_Chest_Sorter__Assessment_": 0.53922867537626,
            "nunique_title": 0.5384794079860751,
            "total_accuracy_Cauldron_Filler__Assessment_": 0.5384722290449699,
            "nunique_world": 0.538430991368872,
            "total_accuracy": 0.5383023585504764,
            "nunique_event_count": 0.5357949377553428,
            "nunique_hour": 0.5354179109896753,
            "total_accuracy_7day": 0.5353800571572359,
            "num_incorrect_title_median": 0.5332139589713053,
            "num_incorrect_title_mean": 0.5321399197439859,
            "num_incorrect_mean": 0.5319127043910482,
            "num_correct_title_median": 0.5282492783149,
            "num_correct_title_mean": 0.5235170400522697,
            "num_correct_title_last": 0.5210077529473687,
            "num_correct_title_7day_last": 0.5135649666685463,
            "num_correct_title_7day_mean": 0.5107151900127229,
            "accuracy_group_title_sum": 0.5086822885225738,
            "num_correct_mean_7day": 0.506361322685065,
            "num_correct_mean": 0.5017461106993205,
            "accuracy_group_title_7day_last": 0.5010894094717081,
            "accuracy_group_title_max": 0.4980199777850703,
            "Ratio_TREETOPCITY_Counter": 0.4977807989095394,
            "accuracy_group_title_7day_mean": 0.49579539077197554,
            "accuracy_group_median": 0.4954865376939462,
            "Ratio_Tree_Top_City___Level_3_2000_Counter": 0.4953887193068187,
            "Ratio_Tree_Top_City___Level_1_2000_Counter": 0.49527893992832983,
            "accuracy_group_mean": 0.4949727125779302,
            "accuracy_group_title_last": 0.4944186434333173,
            "accuracy_group_title_mean": 0.4943618206579238,
            "accuracy_group_mean_7day": 0.4939379966342564,
            "accuracy_group_title_median": 0.4939277938868371,
            "Ratio_Scrub_A_Dub_4010_Counter": 0.4936844400587823,
            "Ratio_Scrub_A_Dub_3020_Counter": 0.48729267671860493,
            "Ratio_Sandcastle_Builder__Activity__4020_Counter": 0.48619810272846165,
            "Ratio_Ordering_Spheres_2000_Counter": 0.4786063459785777,
            "Ratio_Happy_Camel_4070_Counter": 0.47791170296671226,
            "Ratio_Mushroom_Sorter__Assessment__2010_Counter": 0.4766675520505874,
            "Ratio_Happy_Camel_4035_Counter": 0.47640976400934704,
            "Ratio_Game_Counter": 0.47436799184726197,
            "Ratio_Crystal_Caves___Level_3_2000_Counter": 0.4741326875014177,
            "Ratio_Crystal_Caves___Level_2_2000_Counter": 0.4690856762717597,
            "Ratio_Crystal_Caves___Level_1_2000_Counter": 0.46844620434124096,
            "Ratio_Chow_Time_4070_Counter": 0.46734156569310237,
            "Ratio_Chest_Sorter__Assessment__4025_Counter": 0.46258621879962625,
            "Ratio_Chest_Sorter__Assessment__4030_Counter": 0.46228582695921344,
            "Ratio_Cart_Balancer__Assessment__4070_Counter": 0.4599961706261152,
            "Ratio_Chest_Sorter__Assessment__4020_Counter": 0.45912457754455555,
            "Ratio_Cauldron_Filler__Assessment__4025_Counter": 0.4584329013578001,
            "Ratio_Chest_Sorter__Assessment__3021_Counter": 0.45775405415287124,
            "Ratio_CRYSTALCAVES_Counter": 0.4496917418860725,
            "Ratio_Bug_Measurer__Activity__4035_Counter": 0.4484789843004255,
            "Ratio_Bottle_Filler__Activity__4070_Counter": 0.4446941713598608,
            "Ratio_Bird_Measurer__Assessment__2030_Counter": 0.4377579777077343,
            "Ratio_Activity_Counter": 0.4367819415773403,
            "Ratio_Bird_Measurer__Assessment__3121_Counter": 0.4360382559392321,
            "Ratio_All_Star_Sorting_3120_Counter": 0.4357135514632107,
            "Ratio_Air_Show_2030_Counter": 0.4344163688778829,
            "Ratio_4090_Counter": 0.4327704381444802,
            "Ratio_4070_Counter": 0.42702566286376786,
            "Ratio_4035_Counter": 0.41927629208661205,
            "Ratio_4030_Counter": 0.41697280474582765,
            "Ratio_4020_Counter": 0.4094028252414139,
            "Ratio_4010_Counter": 0.4065531893583617,
            "Ratio_3121_Counter": 0.4064003952349687,
            "Ratio_4025_Counter": 0.40506835887819526,
            "Ratio_3120_Counter": 0.4021847497914751,
            "Ratio_3110_Counter": 0.399633746238276,
            "Ratio_3021_Counter": 0.3990668142533706,
            "Ratio_3020_Counter": 0.39618365247519644,
            "Ratio_2000_Counter": 0.3927737734277539,
            "Crystal_Caves___Level_3_Counter": 0.3926014780538819,
            "Crystal_Caves___Level_3_2000_Counter": 0.38611029440152655,
            "Ratio_2010_Counter": 0.3857613499604421,
            "Ratio_3010_Counter": 0.38521471978770216,
            "Ratio_2030_Counter": 0.384070402526698,
            "Clip_Counter": 0.3835423005697541,
            "CRYSTALCAVES_Counter": 0.37715831143182477,
            "Chow_Time_4070_Counter": 0.3766387072823933,
            "4070_Counter": 0.3721856716537313,
            "3020_Counter": 0.3716211781459543,
            "2000_Counter": 0.3667574363314461,
            "mean_target": 0.3594295723905834,
            "Chest Sorter (Assessment)_success_ratio": 0.07081887428801766,
            "Mushroom Sorter (Assessment)_success_ratio": 0.06906269250737251,
            "Sandcastle Builder (Activity)_duration": 0.06722868916430191,
            "var_action_time_Scrub-A-Dub": 0.06459140998114024,
            "mean_action_time_Scrub-A-Dub": 0.06282268235098862,
            "success_ratio_Scrub-A-Dub": 0.06119584226995804,
            "last_success_ratio_Pan Balance": 0.05503962500888684,
            "success_ratio_Pan Balance": 0.05090951786728324,
            "mean_correct_Leaf Leader": 0.050481068682315874,
            "mean_incorrect_Pan Balance": 0.0502621822018938,
            "n_last_correct_Leaf Leader": 0.04968461734524674,
            "mean_action_time_Happy Camel": 0.04552826141868647,
            "last_success_ratio_Happy Camel": 0.04390296152538453,
            "success_ratio_Happy Camel": 0.04302844870128362,
            "mean_correct_Chow Time": 0.04025725402339042,
            "success_ratio_Chow Time": 0.039438581646925555,
            "mean_action_time_Dino Drink": 0.03919885841554038,
            "last_success_ratio_Crystals Rule": 0.03881209225694846,
            "mean_4070_Chow Time": 0.038016496284953315,
            "mean_action_time_All Star Sorting": 0.03516695493795201,
            "last_success_ratio_All Star Sorting": 0.034277680780946215,
            "success_ratio_All Star Sorting": 0.034087769053374316,
            "Chest Sorter (Assessment)_time_to_get_success": 0.031758574943701844,
            "mean_incorrect_All Star Sorting": 0.03037870921438719,
            "success_ratio_Air Show": 0.030373914036219606,
            "Cauldron Filler (Assessment)_3020_mean": 0.030296020178466065,
            "Chest Sorter (Assessment)_mean_action_time": 0.03007989800440656,
            "Chest Sorter (Assessment)_mean_var_action_time": 0.029243361732937533,
            "Cauldron Filler (Assessment)_mean_var_action_time": 0.026865703086375835,
            "Cauldron Filler (Assessment)_mean_action_time": 0.02669069967533242,
            "decayed_success_ratio_last_same_assess": 0.02302620776326738,
            "decayed_n_failure_last_same_assess": 0.02178829018896362,
            "Mushroom Sorter (Assessment)_accuracy_group": 0.02175249350322599,
            "decayed_accuracy_group_last_same_assess": 0.02111870525101258,
            "mean_action_time_last_same_assess": 0.020960023613504665,
            "success_ratio_last_same_assess": 0.020496108998444694,
            "var_time_to_get_success_same_assess": 0.01982141276619511,
            "success_ratio_same_assess": 0.019086764479539275,
            "mean_accuracy_group_same_assess": 0.019064505395397592,
            "n_failure_same_assess": 0.018459457087963704,
            "mean_timte_to_get_success_same_assess": 0.018314090132222537,
            "accumulated_accuracy_group": 0.014473311423128288,
            "world": 0.01426381871721245,
            "accumulated_acc": 0.013696034882748686,
            "session_title": 0.011941764978655844,
            "Crystal Caves - Level 3_2000": 0.011253652100916644,
            "Chow Time_4070": 0.009870020848067807,
            "27253bdc": 0.008334974255393734,
            "3120": 0.005780779355662391,
            "4035": 0.005316866460245384,
            "4070": 0.005210604317610379,
            "2000": 0.00038512087136926
        }
    },
    "truncated_mean_adjust": 0.5721196642955719,
    "truncated_std_adjust": 0.02283692983145139,
    "truncated_upper": 0.6177935239584748,
    "truncated_lower": 0.5264458046326691
}